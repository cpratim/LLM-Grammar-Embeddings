@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Lv, Shuai and Peng, Sheng and Wang, Yida and Zhang, Xingjian and Yang, Ziyue and Yang, Beilei and Gong, Haotian and Fu, Zhiyu and Liu, Kongming and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{linzen2016assessing,
  title={Assessing the ability of LSTMs to learn syntax-sensitive dependencies},
  author={Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  journal={Transactions of the Association for Computational Linguistics},
  volume={4},
  pages={521--535},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{goldberg2019assessing,
  title={Assessing BERT's syntactic abilities},
  author={Goldberg, Yoav},
  booktitle={arXiv preprint arXiv:1901.05287},
  year={2019}
}

@article{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019}
}

@inproceedings{warstadt2019neural,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  booktitle={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  organization={MIT Press}
}

@article{warstadt2020blimp,
  title={BLiMP: The benchmark of linguistic minimal pairs for English},
  author={Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={377--392},
  year={2020},
  publisher={MIT Press}
}

@article{warstadt2020linguistic,
  title={Are linguistic acceptability judgments reliable?},
  author={Warstadt, Alex and Bowman, Samuel R},
  journal={Language},
  volume={96},
  number={3},
  pages={e107--e119},
  year={2020}
}

@inproceedings{thrush2022winograd,
  title={Winoground: Probing vision and language models for visio-linguistic compositionality},
  author={Thrush, Tristan and Jiang, Sanjay and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5238--5248},
  year={2022}
}

@article{qian2022limitations,
  title={Limitations of language models in arithmetic and symbolic reasoning},
  author={Qian, Peng and Huang, Tahsina and Firoozi, Reza and Wang, Zhiyuan and Zhou, Qiyang and Wong, Eric and Chen, Kevin and Pan, Shaopeng and Yu, Zhou and Xiang, Yang and others},
  journal={arXiv preprint arXiv:2208.05051},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{wei2021frequency,
  title={Frequency effects on syntactic rule learning in transformers},
  author={Wei, Jerry and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2109.07020},
  year={2021}
}

@article{zhang2023language,
  title={Language modeling with reduced spurious correlations},
  author={Zhang, Hugh and Webb, Amy and Petryk, Saujas and Han, Yiheng and Lei, Jason and Finn, Chelsea},
  journal={arXiv preprint arXiv:2306.01708},
  year={2023}
}

@article{hu2020systematic,
  title={Systematic evaluation of causal discovery in visual model based reinforcement learning},
  author={Hu, Weihs and Daum√© III, Hal},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12578--12590},
  year={2020}
}

@article{patel2022mapping,
  title={Mapping language models to grounded conceptual spaces},
  author={Patel, Krishna and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2210.02539},
  year={2022}
}

@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@inproceedings{tenney2019bert,
  title={BERT rediscovers the classical NLP pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}

@article{manning2020emergent,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Manning, Christopher D and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30046--30054},
  year={2020},
  publisher={National Acad Sciences}
}

@inproceedings{geva2021transformer,
  title={Transformer feed-forward layers are key-value memories},
  author={Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5484--5495},
  year={2021}
}

@article{levy2018emergence,
  title={Emergence of language with multi-agent games: Learning to communicate with sequences of symbols},
  author={Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Blundell, Charles},
  journal={arXiv preprint arXiv:1705.11192},
  year={2018}
}

@inproceedings{clark2019does,
  title={What does BERT look at? An analysis of BERT's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  booktitle={Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={276--286},
  year={2019}
}

@article{zhang2022opt,
  title={OPT: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{chen2023unlocking,
  title={Unlocking reasoning in large language models: A survey and empirical study},
  author={Chen, Jie and Du, Xingxuan and Hui, Fuzhi and Tian, Zonghan and He, Jingzhou and Xin, Qizhe and Zhang, Guohao and Peng, Baolin and Zhou, Bowen and Zhang, Xingyu and others},
  journal={arXiv preprint arXiv:2302.00623},
  year={2023}
}

@inproceedings{talmor2020olmpics,
  title={oLMpics-on what language model pre-training captures},
  author={Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
  booktitle={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={743--758},
  year={2020},
  organization={MIT Press}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{artetxe2021efficient,
  title={Efficient large scale language modeling with mixtures of experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Ott, Myle and Shleifer, Sam and Therien, Berkeley and Jain, Manan and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}

@article{roberts2023quantifying,
  title={Quantifying language models' sensitivity to spurious features in prompt design or: How I learned to start worrying about prompt formatting},
  author={Roberts, Adam and Webson, Albert and Larson, Colin and Gao, Leo and Tandon, Niket and Tai, Kai-Wei and Chung, Hyung Won and Raffel, Colin and Mishra, Gaurav},
  journal={arXiv preprint arXiv:2310.11324},
  year={2023}
}

@article{xia2023structured,
  title={Structured prompting: Scaling in-context learning to 1,000 examples},
  author={Xia, Jiacheng and Li, Songbo and Xu, Haozhao and Chen, Danny and Liu, Yang and Cohen, Bill and Zhang, Leyang},
  journal={arXiv preprint arXiv:2212.06713},
  year={2023}
}

@article{jumelet2021language,
  title={Language models as psycholinguistic subjects: Syntactic state representations in BERT},
  author={Jumelet, Jaap and van Noord, Gertjan and Hupkes, Dieuwke},
  journal={arXiv preprint arXiv:2110.12077},
  year={2021}
}

@article{lasri2022probing,
  title={Probing for grammar gems: Parametric context probing for freezing effects},
  author={Lasri, Kwaku and Chiang, Cheng-Han and Ettinger, Allyson},
  journal={arXiv preprint arXiv:2210.17195},
  year={2022}
}