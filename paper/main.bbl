\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2023)Bai, Lv, Peng, Wang, Zhang, Yang, Yang, Gong, Fu, Liu, et~al.]{bai2023qwen}
Jinze Bai, Shuai Lv, Sheng Peng, Yida Wang, Xingjian Zhang, Ziyue Yang, Beilei Yang, Haotian Gong, Zhiyu Fu, Kongming Liu, et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and Manning]{clark2019does}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D Manning.
\newblock What does bert look at? an analysis of bert's attention.
\newblock In \emph{Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP}, pages 276--286, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2019.

\bibitem[Geva et~al.(2021)Geva, Schuster, Berant, and Levy]{geva2021transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 5484--5495, 2021.

\bibitem[Goldberg(2019)]{goldberg2019assessing}
Yoav Goldberg.
\newblock Assessing bert's syntactic abilities.
\newblock In \emph{arXiv preprint arXiv:1901.05287}, 2019.

\bibitem[Hewitt and Manning(2019)]{hewitt2019structural}
John Hewitt and Christopher~D Manning.
\newblock A structural probe for finding syntax in word representations.
\newblock In \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4129--4138, 2019.

\bibitem[Hu and Daumé~III(2020)]{hu2020systematic}
Weihs Hu and Hal Daumé~III.
\newblock Systematic evaluation of causal discovery in visual model based reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 12578--12590, 2020.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Lazaridou et~al.(2018)Lazaridou, Hermann, Tuyls, and Blundell]{levy2018emergence}
Angeliki Lazaridou, Karl~Moritz Hermann, Karl Tuyls, and Charles Blundell.
\newblock Emergence of language with multi-agent games: Learning to communicate with sequences of symbols.
\newblock \emph{arXiv preprint arXiv:1705.11192}, 2018.

\bibitem[Linzen et~al.(2016)Linzen, Dupoux, and Goldberg]{linzen2016assessing}
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
\newblock Assessing the ability of lstms to learn syntax-sensitive dependencies.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 4:\penalty0 521--535, 2016.

\bibitem[Manning et~al.(2020)Manning, Clark, Hewitt, Khandelwal, and Levy]{manning2020emergent}
Christopher~D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy.
\newblock Emergent linguistic structure in artificial neural networks trained by self-supervision.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (48):\penalty0 30046--30054, 2020.

\bibitem[{OpenAI}(2023)]{openai2023gpt4}
{OpenAI}.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Patel and Pavlick(2022)]{patel2022mapping}
Krishna Patel and Ellie Pavlick.
\newblock Mapping language models to grounded conceptual spaces.
\newblock \emph{arXiv preprint arXiv:2210.02539}, 2022.

\bibitem[Qian et~al.(2022)Qian, Huang, Firoozi, Wang, Zhou, Wong, Chen, Pan, Yu, Xiang, et~al.]{qian2022limitations}
Peng Qian, Tahsina Huang, Reza Firoozi, Zhiyuan Wang, Qiyang Zhou, Eric Wong, Kevin Chen, Shaopeng Pan, Zhou Yu, Yang Xiang, et~al.
\newblock Limitations of language models in arithmetic and symbolic reasoning.
\newblock \emph{arXiv preprint arXiv:2208.05051}, 2022.

\bibitem[Roberts et~al.(2023)Roberts, Webson, Larson, Gao, Tandon, Tai, Chung, Raffel, and Mishra]{roberts2023quantifying}
Adam Roberts, Albert Webson, Colin Larson, Leo Gao, Niket Tandon, Kai-Wei Tai, Hyung~Won Chung, Colin Raffel, and Gaurav Mishra.
\newblock Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting.
\newblock \emph{arXiv preprint arXiv:2310.11324}, 2023.

\bibitem[Talmor et~al.(2020)Talmor, Elazar, Goldberg, and Berant]{talmor2020olmpics}
Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant.
\newblock olmpics-on what language model pre-training captures.
\newblock In \emph{Transactions of the Association for Computational Linguistics}, volume~8, pages 743--758. MIT Press, 2020.

\bibitem[Tenney et~al.(2019)Tenney, Das, and Pavlick]{tenney2019bert}
Ian Tenney, Dipanjan Das, and Ellie Pavlick.
\newblock Bert rediscovers the classical nlp pipeline.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4593--4601, 2019.

\bibitem[Thrush et~al.(2022)Thrush, Jiang, Bartolo, Singh, Williams, Kiela, and Ross]{thrush2022winograd}
Tristan Thrush, Sanjay Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross.
\newblock Winoground: Probing vision and language models for visio-linguistic compositionality.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5238--5248, 2022.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt2019neural}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock In \emph{Transactions of the Association for Computational Linguistics}, volume~7, pages 625--641. MIT Press, 2019.

\bibitem[Warstadt et~al.(2020)Warstadt, Parrish, Liu, Mohananey, Peng, Wang, and Bowman]{warstadt2020blimp}
Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel~R Bowman.
\newblock Blimp: The benchmark of linguistic minimal pairs for english.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 377--392, 2020.

\bibitem[Wei et~al.(2021)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2021frequency}
Jerry Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Frequency effects on syntactic rule learning in transformers.
\newblock \emph{arXiv preprint arXiv:2109.07020}, 2021.

\bibitem[Xia et~al.(2023)Xia, Li, Xu, Chen, Liu, Cohen, and Zhang]{xia2023structured}
Jiacheng Xia, Songbo Li, Haozhao Xu, Danny Chen, Yang Liu, Bill Cohen, and Leyang Zhang.
\newblock Structured prompting: Scaling in-context learning to 1,000 examples.
\newblock \emph{arXiv preprint arXiv:2212.06713}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Webb, Petryk, Han, Lei, and Finn]{zhang2023language}
Hugh Zhang, Amy Webb, Saujas Petryk, Yiheng Han, Jason Lei, and Chelsea Finn.
\newblock Language modeling with reduced spurious correlations.
\newblock \emph{arXiv preprint arXiv:2306.01708}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.

\end{thebibliography}
