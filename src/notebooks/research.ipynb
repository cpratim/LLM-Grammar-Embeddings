{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_memory_report(device):\n",
    "    print(f'Device: {device} [{torch.cuda.get_device_name(device)}]')\n",
    "    free_memory, total_memory = torch.cuda.mem_get_info(device)\n",
    "    \n",
    "    free_memory_gb = free_memory / (1024 ** 3)\n",
    "    total_memory_gb = total_memory / (1024 ** 3)\n",
    "    \n",
    "    print(f\"Free Memory: {free_memory_gb:.2f}/{total_memory_gb:.2f} GB [{free_memory / total_memory * 100:.2f}%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:5 [NVIDIA RTX 6000 Ada Generation]\n",
      "Free Memory: 37.98/47.50 GB [79.95%]\n"
     ]
    }
   ],
   "source": [
    "get_device_memory_report(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_matrix(matrix):\n",
    "    if len(matrix.shape) == 1:\n",
    "        print(\" \".join(f\"{x:.2f}\" for x in matrix))\n",
    "    else:\n",
    "        for row in matrix:\n",
    "            print(\" \".join(f\"{x:.2f}\" for x in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_id=\"meta-llama/Llama-3.2-1B\", device=\"cuda:1\"):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=device,\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "    model.to(device)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_1B, llama_1B_tokenizer = load_model(model_id=\"meta-llama/Llama-3.2-1B\", device=device)\n",
    "llama_1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:5')\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The cat chased the mouse. The mouse ran away. Another word\"\n",
    "\n",
    "tokens = llama_1B_tokenizer(sentence, return_tensors=\"pt\")\n",
    "tokens = {k: v.to(llama_1B.device) for k, v in tokens.items()}\n",
    "\n",
    "print(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = llama_1B(\n",
    "        **tokens,\n",
    "        output_hidden_states=True,\n",
    "        return_dict=True,\n",
    "        output_attentions=True,\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states[0].shape)\n",
    "\n",
    "first_attention_layer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(llama_1B.base_model.layers[0].self_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
      "        attention_mask: Optional[torch.Tensor],\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
      "        input_shape = hidden_states.shape[:-1]\n",
      "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
      "\n",
      "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "\n",
      "        cos, sin = position_embeddings\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "\n",
      "        if past_key_value is not None:\n",
      "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
      "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
      "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "\n",
      "        attention_interface: Callable = eager_attention_forward\n",
      "        if self.config._attn_implementation != \"eager\":\n",
      "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
      "                logger.warning_once(\n",
      "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
      "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
      "                )\n",
      "            else:\n",
      "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
      "\n",
      "        attn_output, attn_weights = attention_interface(\n",
      "            self,\n",
      "            query_states,\n",
      "            key_states,\n",
      "            value_states,\n",
      "            attention_mask,\n",
      "            dropout=0.0 if not self.training else self.attention_dropout,\n",
      "            scaling=self.scaling,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "        return attn_output, attn_weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "print(inspect.getsource(llama_1B.base_model.layers[0].self_attn.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 2048])\n",
      "torch.Size([512, 2048])\n",
      "torch.Size([32, 16, 2048])\n"
     ]
    }
   ],
   "source": [
    "V_matrix = llama_1B.base_model.layers[0].self_attn.v_proj.weight \n",
    "O_matrix = llama_1B.base_model.layers[0].self_attn.o_proj.weight\n",
    "print(O_matrix.shape)\n",
    "print(V_matrix.shape)\n",
    "\n",
    "num_heads = 32\n",
    "embedding_dim = 2048\n",
    "value_matrix_per_head = V_matrix.view(num_heads, -1, embedding_dim)\n",
    "\n",
    "print(value_matrix_per_head.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(value_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 14, 14])\n",
      "1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.86 0.14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.44 0.51 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.46 0.37 0.06 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.13 0.52 0.12 0.20 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.23 0.11 0.08 0.30 0.22 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.14 0.09 0.07 0.29 0.16 0.07 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.10 0.04 0.03 0.09 0.09 0.09 0.52 0.04 0.00 0.00 0.00 0.00 0.00 0.00\n",
      "0.13 0.03 0.02 0.05 0.06 0.07 0.38 0.22 0.04 0.00 0.00 0.00 0.00 0.00\n",
      "0.12 0.01 0.01 0.04 0.02 0.03 0.36 0.28 0.07 0.05 0.00 0.00 0.00 0.00\n",
      "0.11 0.01 0.01 0.03 0.03 0.03 0.20 0.20 0.14 0.21 0.03 0.00 0.00 0.00\n",
      "0.09 0.01 0.01 0.03 0.03 0.04 0.15 0.11 0.10 0.20 0.06 0.15 0.00 0.00\n",
      "0.09 0.00 0.00 0.01 0.01 0.01 0.06 0.03 0.02 0.09 0.08 0.53 0.06 0.00\n",
      "0.07 0.00 0.00 0.00 0.00 0.00 0.02 0.03 0.01 0.04 0.05 0.40 0.35 0.01\n"
     ]
    }
   ],
   "source": [
    "print(outputs.attentions[0].shape)\n",
    "first_attention_head = outputs.attentions[0][0, 0, :, :]\n",
    "\n",
    "# first_attention_head = first_attenion_head.cpu().numpy()\n",
    "row_sums = first_attention_head.sum(axis=1)\n",
    "\n",
    "# attention_value = torch.matmul(first_attention_head, value_layer)\n",
    "\n",
    "pprint_matrix(first_attention_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n",
      "torch.Size([1, 14, 2048])\n"
     ]
    }
   ],
   "source": [
    "hidden_states = outputs.hidden_states\n",
    "for i in range(len(hidden_states)):\n",
    "    print(hidden_states[i].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
